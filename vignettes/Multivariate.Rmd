---
title: "Multivariate"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Multivariate}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, warning=F, message = F}
library(qacOutliers)
```

## What are multivariate outliers? How do you detect them? 

A multivariate outlier is an outlier that can only be detected by looking at two or more variables in combination. The graph below shows examples of multivariate outliers using the `iris` dataset included with base R.

```{r, echo = F, message = F, warning = F}
#getting the outliers
results <- multiOutliers(iris, method = "lof")
row <- as.numeric(results$Row)

library(dplyr)
#getting the one point for labelling
point <-  iris %>% 
  filter(Sepal.Length < 5 & Petal.Width > 1.5)

#graphing
library(ggplot2)
ggplot()+
  geom_point(data = iris, aes(x=Sepal.Length, y=Petal.Width))+
  geom_point(data = point, aes(x=Sepal.Length, y=Petal.Width), color = "red")+
  geom_label(data = point, aes(x=Sepal.Length, y=Petal.Width), label = rownames(point), vjust = -0.7)+
  labs(title = "Sepal Length and Petal Width", x = "Sepal Length (in)", y = "Petal Width (in)")+
  theme_minimal()
```

The red point labelled `1` on the graph is a clear example of a multivariate outlier. This flower has a Sepal Length of 4.9 inches, and a Petal Width of 1.7 inches, both of which seem like normal values for Sepal Length and Petal Width. However, flowers with a Sepal Length of around 5 inches normally have much smaller Petal Widths, as shown on the graph above. While there are other outliers in this dataset, only this outlier has been colored red to draw extra attention to it. 

The outlier in this graph was detected using the LoF method, and more detail about that method can be provided below. This package specifically focuses on four different methods for finding multivariate outliers: kNN, LoF, mahalanobis distance, and iForest. 

## kNN

kNN calculates the distances between a data point and its k-nearest neighbors and assigns an outlier score based on that distance. The principle that guides kNN is that outliers lay far away from their neighbours, so each of the distances is interpreted within that context. Because some variables in the data may have much larger ranges that others (ex. a variable has a range from 1-10 and another has a range of -10000 to 10000), the data is standardized before calculating the distances. 

After each of these distances is calculated, the mean for each row is calculated. The next step involves creating a threshold for declaring a point an outlier. To calculate this threshold, the function takes the mean of each row (after that row's mean has been calculated), and adds 2 times the standard deviation of each row to that number. Outliers are considered any points with a score above the calculated threshold. Because this method relies on numeric variables, all categorical variables are removed. 

Here is an example of the final output from the `multiOutliers` function using the kNN method.

```{r, echo = F, message = F, warning = F}
multiOutliers(mtcarsOutliers, method="knn")
```

### Customizing the `k` parameter

The value `k` tells the function how many points to consider as neighbors when identifying distances between each of the points. The default value, 5, finds the distance between each point the 5 points that are closest to that point. The choice of `k` significantly impacts the results, and smaller values are generally more sensitive to outliers. You can supply your own value of `k`, which may change the results of the function.

```{r}
multiOutliers(mtcarsOutliers, method = "knn", k = 10)
```

### Example Output
When using the kNN method with the default `k=5`, the function returns:

- `Method`: "kNN", indicating the method used.
- `Dataset`: The dataset name.
- `Variables`: The numeric columns considered for outlier detection.
- `Row`: Indices of rows identified as outliers.
- `Score`: Mean kNN distance scores of detected outliers.
- `Message`: A summary message indicating whether outliers were detected.
- `k`: The number of nearest neighbors considered.
- `Data`: Displays the five highest outliers in the data used. 

```{r}
result <- multiOutliers(mtcarsOutliers, method = "knn")
print(result)
```

Here is an example of graphical output from this function. 
```{r}
plot(result)
```


### Notes and Considerations

1. Numeric Data Only: The kNN method requires numeric variables. Non-numeric columns are automatically excluded.

2. Robustness: kNN does not assume a specific distribution of data, so it is robust to non-normality, making it a better tool to handle non-normal data than other outlier detection methods. 

To learn more about kNN and how it's used in multivariate outlier detection, visit these resources: 

- [GeeksforGeeks.com](https://www.geeksforgeeks.org/k-nearest-neighbours/#)
- [Dualitytech.com](https://dualitytech.com/blog/anomaly-detection-k-nearest-neighbors/) 
- [StatQuest](https://www.youtube.com/watch?v=HVXime0nQeI)

## Local outlier factor (LoF)

The Local Outlier Factor (LoF) method detects anomalies by comparing the density of data points in their local neighborhood. Points with significantly lower density than their neighbors are flagged as potential outliers. The [dbscan](https://cran.r-project.org/web/packages/dbscan/readme/README.html) package is used for this implementation, which calculates LoF scores for each data point. Scores above a certain threshold (typically > 1) are indicative of stronger outliers.

LoF is particularly useful for datasets with clusters of varying density, as it considers the local density when assessing outlier scores. It supports both numeric and categorical variables, using Gower distance for mixed data types. This LoF method uses the [cluster](https://cran.r-project.org/web/packages/cluster/index.html) package's daisy (Dissimilarity Matrix Calculation) function to calculate the Gower distance when necessary.

### Customizing the `minPts` Parameter
The LoF method allows customization of the `minPts` parameter, which is the minimum number of points in the local neighborhood. Larger values result in broader neighborhoods and may reduce sensitivity to smaller clusters. `minPts` defaults to 5 if not specified.

You can adjust these parameters to suit your dataset. Here’s an example:
```{r}
multiOutliers(mtcarsOutliers, method = "lof", minPts = 10)
```

### Example Output
When using the LoF method with the default `minPts = 5`, the function returns:

- `Method`: "LoF", indicating the method used.
- `Dataset`: The dataset name.
- `Variables`: The columns considered in the analysis.
- `Row`: Indices of rows identified as outliers.
- `Score`: LoF scores for each detected outlier.
- `Message`: A summary message indicating whether outliers were detected.
- `minPts`: The parameter value used for the local neighborhood.
- `Data`: Displays the five highest outliers in the data used. 

```{r}
result <- multiOutliers(mtcarsOutliers, method = "lof")
result
```

Here is an example of graphical output from this function. 
```{r}
plot(result)
```

### Notes and Considerations
1. Sensitivity to minPts: The choice of `minPts` significantly influences results. A value too small might result in over-sensitivity, while a value too large might overlook smaller clusters of anomalies.

2. Mixed Data Types: If the dataset contains categorical variables, the method automatically switches from Euclidean to Gower distances for calculating pairwise dissimilarities. Ensure the data is properly encoded.

3. Interpreting LoF Scores: Scores greater than 1.5 typically indicate potential outliers. Adjust the threshold based on the characteristics of your dataset.

To learn more about LoF & Gower distance and how it's used in multivariate outlier detection, visit these resources: 

- [Medium.com](https://towardsdatascience.com/local-outlier-factor-lof-algorithm-for-outlier-identification-8efb887d9843) LoF Explained
- [Gower Distance](https://medium.com/analytics-vidhya/gowers-distance-899f9c4bd553) Gower Distance Explained
- [DBSCAN Documentation](https://scikit-learn.org/dev/modules/generated/sklearn.cluster.DBSCAN.html)


## Mahalanobis 

The Mahalanobis distance measures the distance of a point from the center of a multivariate distribution while accounting for the correlation between variables. This method identifies outliers by calculating how far each point is from the data's multivariate mean, considering the covariance matrix of the data. This approach is particularly useful when variables are highly correlated or have different scales.

Before using the Mahalanobis distance, the function automatically selects numeric columns from the dataset. Non-numeric variables are excluded, ensuring compatibility with the method. The distances are then calculated using the [outliers_mahalanobis](https://www.rdocumentation.org/packages/Routliers/versions/0.0.0.3/topics/outliers_mahalanobis) function from the [Routliers](https://cran.r-project.org/web/packages/Routliers/index.html) package. The Mahalanobis distances that are returned by the function represent the distance from the point to the center of the distribution. 

### Customizing the `alpha` parameter

The `alpha` parameter determines the significance level for outlier detection. The default `alpha` value is 0.1, which corresponds to a 95% confidence level. The function calculates the chi-squared distribution for these points, and removes those with distances outside of the 95% confidence interval range. 

Lower values (e.g., `alpha = 0.01`) result in stricter thresholds, identifying fewer points as outliers while higher values are less strict, identifying more observations as outliers. The function will return the five observations with the largest outliers regardless of the value of alpha. You can modify `alpha` as follows:
```{r}
multiOutliers(mtcarsOutliers, method = "mahalanobis", alpha = 0.01)
```
### Example Output
When using the Mahalanobis method with the default `alpha = 0.1`, the function returns:

- `Method`: "mahalanobis", indicating the method used.
- `Dataset`: The dataset name.
- `Variables`: The numeric columns considered.
- `Row`: Indices of rows identified as outliers.
- `Score`: Mahalanobis distance scores of detected outliers.
- `Message`: A summary message indicating whether outliers were detected.
- `Alpha`: The significance level used.
- `Data`: Displays the five highest outliers in the data used. 

```{r}
result <- multiOutliers(mtcarsOutliers, method = "mahalanobis")
print(result)
```

Here is an example of graphical output from this function. 
```{r}
plot(result)
```


### Notes and Considerations

1. Numeric Data Only: The Mahalanobis method requires numeric variables. Non-numeric columns are automatically excluded.

2. Multivariate Normality: This method assumes the data follows a multivariate normal distribution. Deviations from normality or the presence of extreme outliers may affect the results.

To learn more about Mahalanobis distance and how it's used in multivariate outlier detection, visit these resources: 

- [Statisticshowto.com](https://www.statisticshowto.com/mahalanobis-distance/) Mahalanobis Distance Explained
- [Builtin.com](https://builtin.com/data-science/mahalanobis-distance)

## iForest 

Isolation Forest (iForest) is an unsupervised machine learning algorithm designed to detect anomalies in data and is implemented through functions in the  [isotree](https://cran.r-project.org/web/packages/isotree/index.html) package. It works by creating random partitions of the data and measuring how quickly each point can be isolated. Points that are isolated faster (using fewer splits) are more likely to be outliers.

The iForest algorithm is particularly well-suited for handling high-dimensional data and works with both quantitative and categorical variables. It is robust to noise and scales efficiently for large datasets.

### Customizing Parameters
The iForest method allows customization of two main parameters:

`ntrees`: The number of trees in the isolation forest. A higher value increases precision but also computation time. Default is 100.

`n`: The number of points to return as outliers. Default is 5.

Here’s an example of how you can modify these parameters:
```{r}
multiOutliers(mtcarsOutliers, method = "iforest", ntrees = 200, n = 10)
```

### Example Output

When using the iForest method with the default `ntrees = 100` and `n = 5` the function returns:

- `Method`: "iForest", indicating the method used.
- `Dataset`: The dataset name.
- `Variables`: The numeric columns considered.
- `Row`: Indices of rows identified as outliers.
- `Score`: Isolation scores for each detected outlier.
- `Message`: A summary message indicating whether outliers were detected.
- `ntrees`: The number of trees in the isolation forest
- `n`: The number of points to return as outliers
- `Data`: Displays the five highest outliers in the data used.  

```{r}
result <- multiOutliers(mtcarsOutliers, method = "iforest")
result
```

Here is an example of graphical output from this function. 
```{r}
plot(result)
```

### Notes and Considerations

1. Scalability: Isolation Forest is designed to handle large datasets efficiently, making it suitable for high-dimensional data. However, performance may depend on the ntrees parameter, as higher values can increase computation time.

2. No Assumptions on Data Distribution: Unlike some statistical methods, iForest does not assume a specific data distribution. This makes it robust for detecting outliers in diverse datasets.

3. Handles Mixed Data Types: iForest can process both numeric and categorical variables. However, ensure your data is properly encoded or formatted as required by the [isotree](https://cran.r-project.org/web/packages/isotree/index.html) package.

4. Interpretation of Scores: Higher isolation scores indicate stronger anomalies. You may need to determine an appropriate threshold for your dataset when interpreting the results.

To learn more about Isolation Forest and how it's used in multivariate outlier detection, visit these resources: 

- [Medium.com](https://medium.com/@limyenwee_19946/unsupervised-outlier-detection-with-isolation-forest-eab398c593b2) iForest Explained
- [Andy McDonald on YouTube](https://www.youtube.com/watch?v=O9VvmWj-JAk)



